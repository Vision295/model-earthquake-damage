{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd5e20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Learning parameters from TRAIN data...\n",
      "\n",
      "STEP 2: Applying preprocessing to TEST data...\n",
      "-> Applying One-Hot Encoding...\n",
      "-> Applying Target Encoding (using Train maps)...\n",
      "\n",
      "‚úÖ Successfully created 'preprocessed_test_values_final.csv' with 86868 rows.\n",
      "   (Use this file to generate predictions for submission)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. SETUP: Learn from TRAIN Data (Scaler & Target Map) ---\n",
    "print(\"STEP 1: Learning parameters from TRAIN data...\")\n",
    "try:\n",
    "    train_values = pd.read_csv('train_values.csv')\n",
    "    train_labels = pd.read_csv('train_labels.csv')\n",
    "    train_df = pd.merge(train_values, train_labels, on='building_id')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: 'train_values.csv' or 'train_labels.csv' not found. These are needed to learn encoding maps.\")\n",
    "    exit()\n",
    "\n",
    "# Features for fitting\n",
    "X_fit = train_df.drop(['damage_grade', 'building_id'], axis=1)\n",
    "y_fit = train_df['damage_grade']\n",
    "global_mean = y_fit.mean() # For filling NaNs in Target Encoding\n",
    "\n",
    "# Feature Groups\n",
    "CONT_FEATURES = ['age', 'area_percentage', 'height_percentage']\n",
    "ORDINAL_FEATURES = ['count_floors_pre_eq', 'count_families']\n",
    "OHE_FEATURES = [\n",
    "    'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type', \n",
    "    'other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status'\n",
    "]\n",
    "TARGET_ENCODE_FEATURES = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "FEATURES_TO_SCALE = CONT_FEATURES + ORDINAL_FEATURES\n",
    "\n",
    "# A. Fit Scaler (Log age first)\n",
    "X_fit['age_log'] = np.log1p(X_fit['age'])\n",
    "X_fit = X_fit.drop('age', axis=1)\n",
    "FEATURES_TO_SCALE.remove('age')\n",
    "FEATURES_TO_SCALE.append('age_log')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_fit[FEATURES_TO_SCALE])\n",
    "\n",
    "# B. Fit Target Encoding Maps\n",
    "target_maps = {}\n",
    "for col in TARGET_ENCODE_FEATURES:\n",
    "    # Calculate mean damage_grade for each category in TRAIN\n",
    "    target_maps[col] = y_fit.groupby(X_fit[col]).mean().to_dict()\n",
    "\n",
    "# C. Get OHE Columns Structure (to ensure Test has same columns)\n",
    "X_fit_ohe = pd.get_dummies(X_fit[OHE_FEATURES], drop_first=True)\n",
    "ohe_train_columns = X_fit_ohe.columns.tolist() # Keep this list!\n",
    "\n",
    "\n",
    "# --- 2. APPLY TO TEST DATA (Transform Only) ---\n",
    "print(\"\\nSTEP 2: Applying preprocessing to TEST data...\")\n",
    "try:\n",
    "    test_values = pd.read_csv('test_values.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: 'test_values.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "X_test = test_values.drop('building_id', axis=1)\n",
    "test_ids = test_values['building_id']\n",
    "\n",
    "# A. Log Transform & Scaling (Use fitted scaler)\n",
    "X_test['age_log'] = np.log1p(X_test['age'])\n",
    "X_test = X_test.drop('age', axis=1)\n",
    "X_test[FEATURES_TO_SCALE] = scaler.transform(X_test[FEATURES_TO_SCALE])\n",
    "\n",
    "# B. One-Hot Encoding & Column Alignment\n",
    "print(\"-> Applying One-Hot Encoding...\")\n",
    "X_test = pd.get_dummies(X_test, columns=OHE_FEATURES, drop_first=True)\n",
    "\n",
    "# Important: Align columns with Train set (Add missing cols with 0, drop extra cols)\n",
    "# 1. Add missing columns\n",
    "for col in ohe_train_columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "# 2. Reorder/Filter columns to match Train exactly (excluding other numeric cols for now)\n",
    "# We will do a full reindex at the end to be safe, but let's handle dimensionality reduction first.\n",
    "\n",
    "# C. Dimensionality Reduction (Drop redundant cols found in Train analysis)\n",
    "REDUNDANT_COLS_TO_DROP = ['land_surface_condition_t', 'roof_type_q', 'position_t']\n",
    "X_test = X_test.drop(columns=REDUNDANT_COLS_TO_DROP, errors='ignore')\n",
    "\n",
    "# D. Target Encoding (Use learned maps)\n",
    "print(\"-> Applying Target Encoding (using Train maps)...\")\n",
    "for col in TARGET_ENCODE_FEATURES:\n",
    "    # Map values using the Train dictionary\n",
    "    X_test[f'{col}_target_enc'] = X_test[col].map(target_maps[col])\n",
    "    # Fill unseen categories (NaNs) with global mean from Train\n",
    "    X_test[f'{col}_target_enc'] = X_test[f'{col}_target_enc'].fillna(global_mean)\n",
    "    X_test = X_test.drop(col, axis=1)\n",
    "\n",
    "# E. Boolean to Integer\n",
    "bool_cols = X_test.select_dtypes(include=['bool']).columns\n",
    "if not bool_cols.empty:\n",
    "    X_test[bool_cols] = X_test[bool_cols].astype(int)\n",
    "\n",
    "# F. Re-insert ID\n",
    "X_test.insert(0, 'building_id', test_ids)\n",
    "\n",
    "# Save\n",
    "output_filename = 'preprocessed_test_values_final.csv'\n",
    "X_test.to_csv(output_filename, index=False)\n",
    "print(f\"\\n‚úÖ Successfully created '{output_filename}' with {X_test.shape[0]} rows.\")\n",
    "print(f\"   (Use this file to generate predictions for submission)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eabe78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Learning parameters from TRAIN data...\n",
      "\n",
      "STEP 2: Applying preprocessing to TEST data...\n",
      "-> Applying One-Hot Encoding...\n",
      "-> Applying Target Encoding (using Train maps)...\n",
      "\n",
      "‚úÖ Successfully created 'preprocessed_test_values_final_no_reduction.csv' with 86868 rows.\n",
      "   (Use this file to generate predictions for submission)\n"
     ]
    }
   ],
   "source": [
    "#ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú Ï∞®Ïõê Ï∂ïÏÜå ÏóÜÎäî Î≤ÑÏ†Ñ.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. SETUP: Learn from TRAIN Data (Scaler & Target Map) ---\n",
    "print(\"STEP 1: Learning parameters from TRAIN data...\")\n",
    "try:\n",
    "    train_values = pd.read_csv('train_values.csv')\n",
    "    train_labels = pd.read_csv('train_labels.csv')\n",
    "    train_df = pd.merge(train_values, train_labels, on='building_id')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: 'train_values.csv' or 'train_labels.csv' not found. These are needed to learn encoding maps.\")\n",
    "    exit()\n",
    "\n",
    "# Features for fitting\n",
    "X_fit = train_df.drop(['damage_grade', 'building_id'], axis=1)\n",
    "y_fit = train_df['damage_grade']\n",
    "global_mean = y_fit.mean() # For filling NaNs in Target Encoding\n",
    "\n",
    "# Feature Groups\n",
    "CONT_FEATURES = ['age', 'area_percentage', 'height_percentage']\n",
    "ORDINAL_FEATURES = ['count_floors_pre_eq', 'count_families']\n",
    "OHE_FEATURES = [\n",
    "    'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type', \n",
    "    'other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status'\n",
    "]\n",
    "TARGET_ENCODE_FEATURES = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "FEATURES_TO_SCALE = CONT_FEATURES + ORDINAL_FEATURES\n",
    "\n",
    "# A. Fit Scaler (Log age first)\n",
    "X_fit['age_log'] = np.log1p(X_fit['age'])\n",
    "X_fit = X_fit.drop('age', axis=1)\n",
    "FEATURES_TO_SCALE.remove('age')\n",
    "FEATURES_TO_SCALE.append('age_log')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_fit[FEATURES_TO_SCALE])\n",
    "\n",
    "# B. Fit Target Encoding Maps\n",
    "target_maps = {}\n",
    "for col in TARGET_ENCODE_FEATURES:\n",
    "    # Calculate mean damage_grade for each category in TRAIN\n",
    "    # SeriesÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï†ÄÏû•\n",
    "    means = y_fit.groupby(X_fit[col]).mean()\n",
    "    target_maps[col] = means.to_dict()\n",
    "\n",
    "# C. Get OHE Columns Structure (to ensure Test has same columns)\n",
    "X_fit_ohe = pd.get_dummies(X_fit[OHE_FEATURES], drop_first=True)\n",
    "ohe_train_columns = X_fit_ohe.columns.tolist() \n",
    "\n",
    "\n",
    "# --- 2. APPLY TO TEST DATA (Transform Only) ---\n",
    "print(\"\\nSTEP 2: Applying preprocessing to TEST data...\")\n",
    "try:\n",
    "    test_values = pd.read_csv('test_values.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: 'test_values.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "X_test = test_values.drop('building_id', axis=1)\n",
    "test_ids = test_values['building_id']\n",
    "\n",
    "# A. Log Transform & Scaling (Use fitted scaler)\n",
    "X_test['age_log'] = np.log1p(X_test['age'])\n",
    "X_test = X_test.drop('age', axis=1)\n",
    "X_test[FEATURES_TO_SCALE] = scaler.transform(X_test[FEATURES_TO_SCALE])\n",
    "\n",
    "# B. One-Hot Encoding & Column Alignment\n",
    "print(\"-> Applying One-Hot Encoding...\")\n",
    "X_test = pd.get_dummies(X_test, columns=OHE_FEATURES, drop_first=True)\n",
    "\n",
    "# Important: Align columns with Train set (Add missing cols with 0)\n",
    "# 1. Add missing columns\n",
    "for col in ohe_train_columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# [ÏÇ≠Ï†úÎê®] Dimensionality Reduction Î∂ÄÎ∂Ñ Ï†úÍ±∞\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# D. Target Encoding (Use learned maps)\n",
    "print(\"-> Applying Target Encoding (using Train maps)...\")\n",
    "for col in TARGET_ENCODE_FEATURES:\n",
    "    # Map values using the Train dictionary\n",
    "    X_test[f'{col}_target_enc'] = X_test[col].map(target_maps[col])\n",
    "    # Fill unseen categories (NaNs) with global mean from Train\n",
    "    X_test[f'{col}_target_enc'] = X_test[f'{col}_target_enc'].fillna(global_mean)\n",
    "    X_test = X_test.drop(col, axis=1)\n",
    "\n",
    "# E. Boolean to Integer\n",
    "bool_cols = X_test.select_dtypes(include=['bool']).columns\n",
    "if not bool_cols.empty:\n",
    "    X_test[bool_cols] = X_test[bool_cols].astype(int)\n",
    "\n",
    "# F. Re-insert ID\n",
    "X_test.insert(0, 'building_id', test_ids)\n",
    "\n",
    "# Save\n",
    "output_filename = 'preprocessed_test_values_final_no_reduction.csv'\n",
    "X_test.to_csv(output_filename, index=False)\n",
    "print(f\"\\n‚úÖ Successfully created '{output_filename}' with {X_test.shape[0]} rows.\")\n",
    "print(f\"   (Use this file to generate predictions for submission)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f66b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïù¥ÏßÑ Î≥ÄÏàò Í∞úÏàò: 52\n",
      "ÏàòÏπòÌòï Î≥ÄÏàò Í∞úÏàò: 8\n",
      "\n",
      "üöÄ Training Model with Entity Embeddings...\n",
      "Epoch 1/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7175 - loss: 0.6287 - val_accuracy: 0.7438 - val_loss: 0.5760\n",
      "Epoch 2/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7448 - loss: 0.5760 - val_accuracy: 0.7485 - val_loss: 0.5666\n",
      "Epoch 3/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7456 - loss: 0.5725 - val_accuracy: 0.7501 - val_loss: 0.5623\n",
      "Epoch 4/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7469 - loss: 0.5693 - val_accuracy: 0.7505 - val_loss: 0.5601\n",
      "Epoch 5/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7469 - loss: 0.5687 - val_accuracy: 0.7462 - val_loss: 0.5703\n",
      "\n",
      "üöÄ Training Standard Model (No Embedding)...\n",
      "Epoch 1/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7174 - loss: 0.6352 - val_accuracy: 0.7433 - val_loss: 0.5812\n",
      "Epoch 2/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7435 - loss: 0.5794 - val_accuracy: 0.7472 - val_loss: 0.5712\n",
      "Epoch 3/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7455 - loss: 0.5733 - val_accuracy: 0.7494 - val_loss: 0.5660\n",
      "Epoch 4/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7475 - loss: 0.5700 - val_accuracy: 0.7496 - val_loss: 0.5658\n",
      "Epoch 5/5\n",
      "\u001b[1m815/815\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7475 - loss: 0.5679 - val_accuracy: 0.7471 - val_loss: 0.5649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2924e440050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (ÏïûÏÑú Ï†ÑÏ≤òÎ¶¨Îêú 100% Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö© Í∞ÄÏ†ï)\n",
    "# ÎßåÏïΩ ÌååÏùºÏù¥ ÏóÜÎã§Î©¥ Ïù¥Ï†Ñ Îã®Í≥ÑÏùò Ï†ÑÏ≤òÎ¶¨ ÏΩîÎìúÎ•º Î®ºÏ†Ä Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî.\n",
    "try:\n",
    "    df = pd.read_csv('final_features_preprocessed_100_percent_no_reduction.csv')\n",
    "    df_labels = pd.read_csv('train_labels.csv')\n",
    "    \n",
    "    # Train Îç∞Ïù¥ÌÑ∞Îßå Ï∂îÏ∂ú (ÎùºÎ≤®Ïù¥ ÏûàÎäî Îç∞Ïù¥ÌÑ∞)\n",
    "    train_data = pd.merge(df, df_labels, on='building_id')\n",
    "    y = train_data['damage_grade'] - 1 # KerasÎäî 0Î∂ÄÌÑ∞ ÏãúÏûëÌïòÎäî ÌÅ¥ÎûòÏä§Î•º ÏõêÌïòÎØÄÎ°ú 1,2,3 -> 0,1,2Î°ú Î≥ÄÌôò\n",
    "    X = train_data.drop(['damage_grade', 'building_id'], axis=1)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Ïù¥ÏßÑ Î≥ÄÏàòÏôÄ ÏàòÏπòÌòï Î≥ÄÏàò Î∂ÑÎ¶¨\n",
    "# Boolean Ïª¨Îüº(0/1Î°ú ÎêòÏñ¥ÏûàÎäî Í≤ÉÎì§)ÏùÑ Ï∞æÏäµÎãàÎã§.\n",
    "# Ï£ºÏùò: Ïù¥ÎØ∏ 0/1Î°ú Î≥ÄÌôòÎêòÏñ¥ ÏûàÏúºÎØÄÎ°ú, 2Í∞úÏùò Í≥†Ïú†Í∞íÏùÑ Í∞ÄÏßÑ Ïª¨ÎüºÏùÑ Ï∞æÏäµÎãàÎã§.\n",
    "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
    "numerical_cols = [col for col in X.columns if col not in binary_cols]\n",
    "\n",
    "print(f\"Ïù¥ÏßÑ Î≥ÄÏàò Í∞úÏàò: {len(binary_cols)}\")\n",
    "print(f\"ÏàòÏπòÌòï Î≥ÄÏàò Í∞úÏàò: {len(numerical_cols)}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [Î™®Îç∏ 1] Entity EmbeddingÏùÑ ÏÇ¨Ïö©ÌïòÎäî Îî•Îü¨Îãù Î™®Îç∏\n",
    "# ---------------------------------------------------------\n",
    "inputs = []\n",
    "embeddings = []\n",
    "\n",
    "# 1) Ïù¥ÏßÑ Î≥ÄÏàò Ï≤òÎ¶¨ (Embedding Layer)\n",
    "for col in binary_cols:\n",
    "    input_layer = Input(shape=(1,), name=col)\n",
    "    inputs.append(input_layer)\n",
    "    # 2Í∞úÏùò Î≤îÏ£º(0, 1)Î•º 2Ï∞®Ïõê Î≤°ÌÑ∞Î°ú ÏûÑÎ≤†Îî© (input_dim=2, output_dim=2)\n",
    "    embedding = Embedding(input_dim=2, output_dim=2, name=f'{col}_emb')(input_layer)\n",
    "    vec = Flatten()(embedding)\n",
    "    embeddings.append(vec)\n",
    "\n",
    "# 2) ÏàòÏπòÌòï Î≥ÄÏàò Ï≤òÎ¶¨ (Í∑∏ÎåÄÎ°ú ÏûÖÎ†•)\n",
    "if len(numerical_cols) > 0:\n",
    "    num_input = Input(shape=(len(numerical_cols),), name='numerical_input')\n",
    "    inputs.append(num_input)\n",
    "    embeddings.append(num_input)\n",
    "\n",
    "# 3) Î≥ëÌï© Î∞è ÌïôÏäµ\n",
    "x = Concatenate()(embeddings)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output = Dense(3, activation='softmax')(x) # 3Í∞ú ÌÅ¥ÎûòÏä§ ÏòàÏ∏°\n",
    "\n",
    "model_emb = Model(inputs=inputs, outputs=output)\n",
    "model_emb.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (Î¶¨Ïä§Ìä∏ ÌòïÌÉú)\n",
    "def get_keras_input(df):\n",
    "    input_list = [df[col].values for col in binary_cols]\n",
    "    if len(numerical_cols) > 0:\n",
    "        input_list.append(df[numerical_cols].values)\n",
    "    return input_list\n",
    "\n",
    "print(\"\\nüöÄ Training Model with Entity Embeddings...\")\n",
    "model_emb.fit(get_keras_input(X_train), y_train, validation_data=(get_keras_input(X_val), y_val), epochs=5, batch_size=256, verbose=1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [Î™®Îç∏ 2] ÏùºÎ∞òÏ†ÅÏù∏ 0/1 ÏûÖÎ†• (ÎπÑÍµêÍµ∞)\n",
    "# ---------------------------------------------------------\n",
    "# Îã®ÏàúÌûà Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º ÌïòÎÇòÏùò Îç©Ïñ¥Î¶¨Î°ú ÎÑ£Îäî ÏùºÎ∞òÏ†ÅÏù∏ Î∞©Ïãù\n",
    "input_all = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(64, activation='relu')(input_all)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_simple = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model_simple = Model(inputs=input_all, outputs=output_simple)\n",
    "model_simple.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nüöÄ Training Standard Model (No Embedding)...\")\n",
    "model_simple.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da51347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Entity Embedding Preprocessing Script Started...\n",
      "-> Loading Data...\n",
      "-> Categorical Features for Embedding: 33\n",
      "-> Numerical Features: 5\n",
      "-> Training Neural Network to learn embeddings...\n",
      "Epoch 1/5\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.7071 - loss: 0.6466 - val_accuracy: 0.7410 - val_loss: 0.5772\n",
      "Epoch 2/5\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7522 - loss: 0.5459 - val_accuracy: 0.7438 - val_loss: 0.5720\n",
      "Epoch 3/5\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7599 - loss: 0.5243 - val_accuracy: 0.7439 - val_loss: 0.5755\n",
      "Epoch 4/5\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7675 - loss: 0.5065 - val_accuracy: 0.7434 - val_loss: 0.5833\n",
      "Epoch 5/5\n",
      "\u001b[1m408/408\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7751 - loss: 0.4891 - val_accuracy: 0.7417 - val_loss: 0.5932\n",
      "-> Extracting Embeddings and Transforming Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:157: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_train_embedded.insert(0, 'building_id', train_df['building_id'])\n",
      "C:\\Users\\mzonl\\AppData\\Local\\Temp\\ipykernel_45216\\1310952388.py:158: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_test_embedded.insert(0, 'building_id', submission_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Saving Files...\n",
      "‚úÖ 'train_embedded.csv' (without label) and 'test_embedded.csv' created successfully!\n",
      "   Train Shape: (260601, 165)\n",
      "   Test Shape: (86868, 165)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"üöÄ Entity Embedding Preprocessing Script Started...\")\n",
    "\n",
    "# --- 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú ---\n",
    "print(\"-> Loading Data...\")\n",
    "try:\n",
    "    train_values = pd.read_csv('train_values.csv')\n",
    "    train_labels = pd.read_csv('train_labels.csv')\n",
    "    test_values = pd.read_csv('test_values.csv')\n",
    "    \n",
    "    # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î≥ëÌï©\n",
    "    train_df = pd.merge(train_values, train_labels, on='building_id')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "    exit()\n",
    "\n",
    "# ÌÉÄÍ≤ü Î≥ÄÏàò (KerasÎäî 0Î∂ÄÌÑ∞ ÏãúÏûëÌïòÎäî ÌÅ¥ÎûòÏä§Î•º ÏõêÌïòÎØÄÎ°ú 1,2,3 -> 0,1,2Î°ú Î≥ÄÌôò)\n",
    "y = train_df['damage_grade'].values - 1\n",
    "X_train_raw = train_df.drop(['damage_grade', 'building_id'], axis=1)\n",
    "X_test_raw = test_values.drop(['building_id'], axis=1)\n",
    "submission_ids = test_values['building_id']\n",
    "\n",
    "# --- 2. Î≥ÄÏàò Íµ¨Î∂Ñ (Î≤îÏ£ºÌòï vs ÏàòÏπòÌòï) ---\n",
    "# Ïù¥ÏßÑ Î≥ÄÏàò (Binary) - ÏöîÏ≤≠ÌïòÏã† ÎåÄÎ°ú ÏûÑÎ≤†Îî© ÎåÄÏÉÅÏóê Ìè¨Ìï®\n",
    "binary_cols = [col for col in X_train_raw.columns if X_train_raw[col].nunique() == 2]\n",
    "\n",
    "# Í≥†Ï∞®Ïõê Î≤îÏ£ºÌòï (Geo Levels) - ÏûÑÎ≤†Îî© Ìö®Í≥ºÍ∞Ä Í∞ÄÏû• ÌÅº\n",
    "geo_cols = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "\n",
    "# ÎÇòÎ®∏ÏßÄ Î≤îÏ£ºÌòï (Object)\n",
    "object_cols = [col for col in X_train_raw.select_dtypes(include='object').columns]\n",
    "\n",
    "# ÏûÑÎ≤†Îî©ÏùÑ Ï†ÅÏö©Ìï† Î™®Îì† Î≤îÏ£ºÌòï Î≥ÄÏàò Î¶¨Ïä§Ìä∏\n",
    "categorical_cols = geo_cols + object_cols + binary_cols\n",
    "\n",
    "# ÏàòÏπòÌòï Î≥ÄÏàò (ÏûÑÎ≤†Îî© Ï†úÏô∏)\n",
    "numerical_cols = [col for col in X_train_raw.columns if col not in categorical_cols]\n",
    "\n",
    "print(f\"-> Categorical Features for Embedding: {len(categorical_cols)}\")\n",
    "print(f\"-> Numerical Features: {len(numerical_cols)}\")\n",
    "\n",
    "# --- 3. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ (Label Encoding & Scaling) ---\n",
    "# Train/TestÎ•º Ìï©Ï≥êÏÑú Ïù∏ÏΩîÎî© (Î≤îÏ£º ÏùºÏπò ÏúÑÌï®)\n",
    "combined_df = pd.concat([X_train_raw, X_test_raw], axis=0)\n",
    "\n",
    "# Î≤îÏ£ºÌòï: Label Encoding (Í∞Å Î≤îÏ£ºÎ•º 0 ~ N-1 Ï†ïÏàòÎ°ú Î≥ÄÌôò)\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined_df[col] = le.fit_transform(combined_df[col].astype(str))\n",
    "\n",
    "# ÏàòÏπòÌòï: Î°úÍ∑∏ Î≥ÄÌôò Î∞è Ïä§ÏºÄÏùºÎßÅ\n",
    "if 'age' in numerical_cols:\n",
    "    combined_df['age'] = np.log1p(combined_df['age'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "combined_df[numerical_cols] = scaler.fit_transform(combined_df[numerical_cols])\n",
    "\n",
    "# Îã§Ïãú Î∂ÑÎ¶¨\n",
    "X_train_enc = combined_df.iloc[:len(X_train_raw)]\n",
    "X_test_enc = combined_df.iloc[len(X_train_raw):]\n",
    "\n",
    "# --- 4. Keras Î™®Îç∏ Íµ¨ÏÑ± (Entity Embedding ÌïôÏäµÏö©) ---\n",
    "inputs = []\n",
    "embeddings = []\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ï∞®Ïõê ÏÑ§Ï†ï Ìï®Ïàò (Í≤ΩÌóòÏ†Å Í∑úÏπô)\n",
    "def get_embedding_dim(n_unique):\n",
    "    if n_unique <= 2: return 1 # Ïù¥ÏßÑ Î≥ÄÏàòÎäî 1Ï∞®ÏõêÏúºÎ°ú\n",
    "    return min(50, (n_unique + 1) // 2)\n",
    "\n",
    "# Í∞Å Î≤îÏ£ºÌòï Î≥ÄÏàòÏóê ÎåÄÌï¥ ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥ ÏÉùÏÑ±\n",
    "for col in categorical_cols:\n",
    "    n_unique = combined_df[col].nunique()\n",
    "    embed_dim = get_embedding_dim(n_unique)\n",
    "    \n",
    "    input_layer = Input(shape=(1,), name=f'in_{col}')\n",
    "    inputs.append(input_layer)\n",
    "    \n",
    "    # Embedding Layer: Ï†ïÏàò Ïù∏Îç±Ïä§Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "    embedding = Embedding(input_dim=n_unique, output_dim=embed_dim, name=f'emb_{col}')(input_layer)\n",
    "    vec = Flatten()(embedding)\n",
    "    embeddings.append(vec)\n",
    "\n",
    "# ÏàòÏπòÌòï Î≥ÄÏàò Ï∂îÍ∞Ä\n",
    "if len(numerical_cols) > 0:\n",
    "    num_input = Input(shape=(len(numerical_cols),), name='in_numerical')\n",
    "    inputs.append(num_input)\n",
    "    embeddings.append(num_input)\n",
    "\n",
    "# Î™®Îç∏ Íµ¨Ï°∞ Í≤∞Ìï©\n",
    "x = Concatenate()(embeddings)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output = Dense(3, activation='softmax')(x) # 3Í∞ú ÌÅ¥ÎûòÏä§ Î∂ÑÎ•ò\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- 5. Î™®Îç∏ ÌïôÏäµ (ÏûÑÎ≤†Îî© Í∞ÄÏ§ëÏπò ÌïôÏäµ) ---\n",
    "print(\"-> Training Neural Network to learn embeddings...\")\n",
    "\n",
    "# ÌïôÏäµÏö© ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±\n",
    "def make_input_dict(df):\n",
    "    d = {f'in_{col}': df[col].values for col in categorical_cols}\n",
    "    if len(numerical_cols) > 0:\n",
    "        d['in_numerical'] = df[numerical_cols].values\n",
    "    return d\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏùºÎ∂ÄÎ•º Í≤ÄÏ¶ùÏö©ÏúºÎ°ú Î∂ÑÌï†ÌïòÏó¨ ÌïôÏäµ\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train_enc, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(make_input_dict(X_tr), y_tr, \n",
    "          validation_data=(make_input_dict(X_val), y_val), \n",
    "          epochs=5, batch_size=512, verbose=1)\n",
    "\n",
    "# --- 6. ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ï∂îÏ∂ú Î∞è Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ---\n",
    "print(\"-> Extracting Embeddings and Transforming Data...\")\n",
    "\n",
    "def replace_with_embeddings(df_enc):\n",
    "    new_df = pd.DataFrame(index=df_enc.index)\n",
    "    \n",
    "    # ÏàòÏπòÌòïÏùÄ Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨\n",
    "    for col in numerical_cols:\n",
    "        new_df[col] = df_enc[col]\n",
    "        \n",
    "    # Î≤îÏ£ºÌòïÏùÄ ÌïôÏäµÎêú ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î°ú ÍµêÏ≤¥\n",
    "    for col in categorical_cols:\n",
    "        layer_name = f'emb_{col}'\n",
    "        # ÌïôÏäµÎêú Í∞ÄÏ§ëÏπò(Î≤°ÌÑ∞ ÌÖåÏù¥Î∏î) Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        weights = model.get_layer(layer_name).get_weights()[0] \n",
    "        \n",
    "        # Îç∞Ïù¥ÌÑ∞(Ïù∏Îç±Ïä§)Î•º Î≤°ÌÑ∞Î°ú Îß§Ìïë\n",
    "        indices = df_enc[col].values\n",
    "        embedded_values = weights[indices]\n",
    "        \n",
    "        # Î≤°ÌÑ∞Ïùò Í∞Å Ï∞®ÏõêÏùÑ Î≥ÑÎèÑÏùò Ïª¨ÎüºÏúºÎ°ú Ï∂îÍ∞Ä\n",
    "        embed_dim = weights.shape[1]\n",
    "        for i in range(embed_dim):\n",
    "            new_df[f'{col}_emb_{i}'] = embedded_values[:, i]\n",
    "            \n",
    "    return new_df\n",
    "\n",
    "X_train_embedded = replace_with_embeddings(X_train_enc)\n",
    "X_test_embedded = replace_with_embeddings(X_test_enc)\n",
    "\n",
    "# --- 7. Ï†ÄÏû• ---\n",
    "print(\"-> Saving Files...\")\n",
    "\n",
    "# ID Î≥µÍµ¨\n",
    "X_train_embedded.insert(0, 'building_id', train_df['building_id'])\n",
    "X_test_embedded.insert(0, 'building_id', submission_ids)\n",
    "\n",
    "# [ÏàòÏ†ïÎê®] damage_grade Ïª¨Îüº Ï∂îÍ∞ÄÌïòÏßÄ ÏïäÏùå\n",
    "# X_train_embedded['damage_grade'] = y + 1 \n",
    "\n",
    "X_train_embedded.to_csv('train_embedded.csv', index=False)\n",
    "X_test_embedded.to_csv('test_embedded.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ 'train_embedded.csv' (without label) and 'test_embedded.csv' created successfully!\")\n",
    "print(f\"   Train Shape: {X_train_embedded.shape}\")\n",
    "print(f\"   Test Shape: {X_test_embedded.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
