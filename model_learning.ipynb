{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b92599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train_values.csv\")\n",
    "\n",
    "categorical_columns = [\n",
    "     \"roof_type               \",\n",
    "     \"land_surface_condition  \",\n",
    "     \"legal_ownership_status  \",\n",
    "     \"other_floor_type        \",\n",
    "     \"position                \",\n",
    "     \"foundation_type         \",\n",
    "     \"ground_floor_type       \",\n",
    "     \"count_floors_pre_eq     \",\n",
    "     \"count_families          \",\n",
    "     \"plan_configuration      \" \n",
    "]\n",
    "\n",
    "bool_columns = [\n",
    "      \"has_superstructure_adobe_mud                \",\n",
    "      \"has_superstructure_bamboo                   \",\n",
    "      \"has_secondary_use_rental                    \", \n",
    "      \"has_secondary_use_hotel                     \", \n",
    "      \"has_secondary_use                           \",  \n",
    "      \"has_secondary_use_agriculture               \", \n",
    "      \"has_superstructure_other                    \", \n",
    "      \"has_superstructure_rc_engineered            \",  \n",
    "      \"has_superstructure_rc_non_engineered        \",  \n",
    "      \"has_superstructure_cement_mortar_stone      \",  \n",
    "      \"has_superstructure_timber                   \",  \n",
    "      \"has_superstructure_cement_mortar_brick      \",  \n",
    "      \"has_superstructure_mud_mortar_brick         \",  \n",
    "      \"has_superstructure_mud_mortar_stone         \",  \n",
    "      \"has_superstructure_stone_flag               \",  \n",
    "      \"has_secondary_use_institution               \",  \n",
    "      \"has_secondary_use_health_post               \",  \n",
    "      \"has_secondary_use_other                     \",  \n",
    "      \"has_secondary_use_use_police                \",  \n",
    "      \"has_secondary_use_gov_office                \",  \n",
    "      \"has_secondary_use_school                    \",  \n",
    "      \"has_secondary_use_industry                  \"  \n",
    "]\n",
    "categorical_columns = list(map(lambda x: x.strip(), categorical_columns))\n",
    "bool_columns = list(map(lambda x: x.strip(), bool_columns))\n",
    "\n",
    "df[categorical_columns] = df[categorical_columns].astype(\"category\")\n",
    "df[bool_columns] = df[bool_columns].astype(\"bool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894af08a",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "### base line model with bagging algorithm\n",
    "\n",
    "1. Does not require data scaling\n",
    "2. Less chance of overfitting\n",
    "3. Not highly damaged from small noise or outliers\n",
    "4. Easily find importance of columns\n",
    "\n",
    "1. Weak for catching patterns than boosting algorithms\n",
    "2. Requires large memories\n",
    "3. Can take **ONLY** float(or integer) type, never categorical or boolean, string itself. They need to be encoded to numeric.\n",
    "\n",
    "Hyperparameters\n",
    "| Name | Role | Effect When Increasing / Decreasing | dataset characteristic |\n",
    "|------|-------|-------------------------------------|-----------------------|\n",
    "| n_estimators | Number of trees | Increase: more stable, lower variance, slower.<br> Decrease: faster but higher variance. | more noise, higher this parameter |\n",
    "| max_depth | Maximum depth of each tree | Increase: deeper, more complex, more overfitting.<br> Decrease: simpler, less overfitting, higher bias. | - |\n",
    "| min_samples_split | Minimum samples to split a node | Increase: fewer splits, simpler trees.<br> Decrease: more splits, deeper trees. | More outliers, highter this parameter |\n",
    "| min_samples_leaf | Minimum samples per leaf | Increase: smoother prediction, less overfitting.<br> Decrease: more complex tree, more overfitting. | For imbalanced dataset, it should be low enough to capture minority classes |\n",
    "| max_features | Features considered at each split | Increase: trees become similar, lower randomness.<br> Decrease: higher randomness, better generalization. | Once important features are well known, higher this parameter |\n",
    "| max_leaf_nodes | Maximum leaf nodes | Increase: more complex trees.<br> Decrease: simpler trees. | - |\n",
    "| max_samples | Samples per tree (with bootstrap) | Increase: more data per tree, less randomness.<br> Decrease: faster, more randomness. | For very large dataset, decrease for speed |\n",
    "| class_weight | Class imbalance handling | Higher minority weight: better recall, lower precision.<br> Lower weight: can ignore minority. | For imbalanced dataset, 'balanced' can take minority class work better |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97fb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning...\n",
      "Best Parameters: {'class_weight': 'balanced', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best CV Score: 0.6603\n",
      "\n",
      "[Test Set Evaluation]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.66      0.59      5025\n",
      "           2       0.75      0.74      0.75     29652\n",
      "           3       0.70      0.66      0.68     17444\n",
      "\n",
      "    accuracy                           0.71     52121\n",
      "   macro avg       0.66      0.69      0.67     52121\n",
      "weighted avg       0.71      0.71      0.71     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.drop(columns=categorical_columns)\n",
    "y = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Initialization\n",
    "# ==========================================\n",
    "# n_jobs=-1 uses all available CPU cores\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Hyperparameter Grid Definition\n",
    "# ==========================================\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],              # Number of trees\n",
    "    'min_samples_leaf': [1, 2, 4],           # Min samples at leaf node\n",
    "    'max_features': ['sqrt', 'log2'],        # Features to consider at split\n",
    "    'class_weight': [None, 'balanced'],      # Handling imbalance\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Grid Search Execution\n",
    "# ==========================================\n",
    "gs = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,                 # 5-Fold Cross Validation\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,            # Parallel processing\n",
    ")\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results & Evaluation\n",
    "# ==========================================\n",
    "print(f\"Best Parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV Score: {gs.best_score_:.4f}\")\n",
    "\n",
    "# Predict using the best model found\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\n[Test Set Evaluation]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# local CV Score : 0.6603"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94dc42",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7ab6c",
   "metadata": {},
   "source": [
    "### Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d40aa",
   "metadata": {},
   "source": [
    "1. Best performance on tabular data\n",
    "2. highly customizable hyperparameters\n",
    "3. Many blackbox algorithms for effetiveness and stability\n",
    "4. Highly customizable, much difficult to use\n",
    "5. Very slow between boosting models\n",
    "6. Very weak for outliers(since this algorithm makes tree based on what previous tree failed to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f023f9",
   "metadata": {},
   "source": [
    "| Parameter | Description | If Increased | If Decreased | When You Should Tune It |\n",
    "|----------|-------------|--------------|--------------|---------------------------|\n",
    "| **learning_rate (eta)** | Step size shrinkage. | Faster learning, but can overfit or become unstable. | Slower learning, requires more trees, but increases stability. | Always tune; small eta + more trees usually performs best. |\n",
    "| **n_estimators** | Total number of boosting rounds. | More complex model; helps with small eta. | Less complex; may underfit. | Tune together with learning_rate. |\n",
    "| **max_depth** | Maximum depth of trees; controls model complexity. | More complex trees, higher variance, more overfitting. | Simpler trees, less overfitting, might underfit. | When the model overfits or underfits; adjust for dataset complexity. |\n",
    "| **min_child_weight** | Minimum sum of instance weights in a child. | Harder for nodes to split → less complex model. | Easier to split → more complex trees. | When trees overfit; increase it for noisy or small datasets. |\n",
    "| subsample | Fraction of rows sampled for each tree. | Adds randomness → reduces overfitting. Too high may overfit. | Uses fewer samples → risk of underfitting if too low. | For overfitting or extremely large datasets. |\n",
    "| **colsample_bytree** | Fraction of features per tree. | More features per tree → more complexity. | Fewer features → more randomness, less overfitting. | Use for high-dimensional data to prevent overfitting. |\n",
    "| colsample_bylevel | Features per tree level. | More features per split → increased complexity. | Less complexity and more randomness. | Tune only for high-dimensional or sparse data. |\n",
    "| colsample_bynode | Features per split node. | Allows more detailed splits. | Increases randomness, reduces variance. | Useful when dataset is huge or overfitting. |\n",
    "| reg_alpha (L1) | L1 penalty; encourages sparsity. | More regularization → simpler model. | Less regularization → more complex. | Use when data is high-dimensional or noisy. |\n",
    "| reg_lambda (L2) | L2 penalty; smooths weights. | More regularization → reduces variance. | Less → more variance, risk of overfitting. | Tune when model overfits or data is noisy. |\n",
    "| max_leaves | Maximum number of leaves for loss-guided growth. | More complex trees. | Simpler trees. | Tune when using grow_policy=\"lossguide\". |\n",
    "| max_bin | Number of bins for histogram algorithm. | Better split precision → more overfitting risk. | Coarser bins → less precision, may underfit. | For large datasets or speed–accuracy tradeoffs. |\n",
    "| grow_policy | How trees grow: depthwise or lossguide. | lossguide allows deeper, selective splits. | depthwise is shallower but stable. | Tune for large or sparse datasets. |\n",
    "| **scale_pos_weight** | Balances positive/negative classes. | Increases recall for minority class. | Decreases minority emphasis. | When dataset is imbalanced. |\n",
    "| **sampling_method** | Method for subsample: uniform or gradient_based. | gradient_based speeds up training. | uniform is stable. | For large datasets. |\n",
    "| **importance_type** | How feature importance is computed. | N/A | N/A | Non-critical — only for interpretability. |\n",
    "| booster | Type of model: gbtree, dart, gblinear. | dart adds dropout → reduces overfitting. | gblinear simpler model. | Change when gbtree fails or data is linear. |\n",
    "| **early_stopping_rounds** | Stops training when no improvement occurs. | Stops earlier → prevents overfitting. | Allows more boosting rounds. | Always useful when using validation sets. |\n",
    "| **tree_method** | Algorithm for tree building. | gpu_hist speeds up massively. | exact is slow but precise. | Tune based on hardware and dataset size. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8347eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     48\u001b[39m gs = GridSearchCV(\n\u001b[32m     49\u001b[39m     estimator=xgb,\n\u001b[32m     50\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     verbose=\u001b[32m2\u001b[39m           \n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Hyperparameter Tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43mgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 5. Results & Evaluation\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgs.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dongw\\anaconda3\\envs\\study\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.drop(columns=categorical_columns)\n",
    "y_raw = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Initialization\n",
    "# ==========================================\n",
    "# tree_method='hist': faster for large datasets\n",
    "# device='cuda': use gpu if available\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softmax', \n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_depth=15,\n",
    "    min_child_weight=5,\n",
    "    tree_method='hist',         \n",
    "    device='cuda'            \n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Hyperparameter Grid Definition\n",
    "# ==========================================\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],          # number of trees\n",
    "    'scale_pos_weight' : [0.8, 1.0],\n",
    "    'learning_rate': [0.1, 0.05],        # low -> slower but more precise\n",
    "    #'gamma': [0, 1]                     # if overfitting occurs, 1\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Grid Search Execution\n",
    "# ==========================================\n",
    "gs = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2           \n",
    ")\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results & Evaluation\n",
    "# ==========================================\n",
    "print(f\"Best Parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV Score: {gs.best_score_:.4f}\")\n",
    "\n",
    "# Predict using the best model found\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "y_test_org = le.inverse_transform(y_test)\n",
    "y_pred_org = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"\\n[Test Set Evaluation]\")\n",
    "print(classification_report(y_test_org, y_pred_org))\n",
    "\n",
    "# local CV Score : 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3ae4b",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "\n",
    "### Light Version of Gradient Boosting, much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad8a9a",
   "metadata": {},
   "source": [
    "1. much faster than XGBoost\n",
    "2. works for more than 100k dataset\n",
    "3. Take Categorical columns and encode in best way, with black-box algorithm\n",
    "4. weak for overfitting\n",
    "5. For smaller training, lower performance than XGBoost\n",
    "\n",
    "| Parameter | Description | If Increased | If Decreased | When You Should Tune It |\n",
    "|:---|:---|:---|:---|:---|\n",
    "| **num_leaves** | **[Most Critical]** Max number of leaves in one tree. Controls model complexity. | • Higher accuracy.<br>• **High risk of Overfitting.** | • Simpler model.<br>• Risk of Underfitting.<br>• Better generalization. | **Always.** Keep it $< 2^{max\\_depth}$. |\n",
    "| **min_data_in_leaf**<br>*(min_child_samples)* | Minimum number of samples required in a leaf node. | • **Prevents overfitting.**<br>• Tree becomes conservative.<br>• Ignores noise. | • Allows learning specific patterns.<br>• **High Overfitting risk.** | Increase this (e.g., 100~1000) to stop the model from learning noise. |\n",
    "| **max_depth** | Limit the depth of the tree explicitly. | • More complex model.<br>• Captures deep interactions. | • Prevents overfitting.<br>• Saves memory. | Use it as a **limit** (e.g., 10~15) to prevent `num_leaves` from exploding. |\n",
    "| **learning_rate** | Step size shrinkage used in update. | • Faster training.<br>• Lower accuracy (needs early stopping). | • Slower training.<br>• **Higher accuracy.** | **Always.** Start with 0.1 for speed, tune with 0.01~0.05 for final result. |\n",
    "| **n_estimators**<br>*(num_iterations)* | Number of boosting rounds. | • Better performance (up to a point).<br>• Slower training. | • Underfitting risk. | Tune together with `learning_rate`. use high value (1000+) always with **Early Stopping**. |\n",
    "| **colsample_bytree**<br>*(feature_fraction)* | Fraction of features (columns) used per iteration. | • Faster training.<br>• **Reduces Overfitting.** | • Slower training.<br>• Uses all features. | Tune if you have many features or one dominant feature overpowers others. |\n",
    "| **subsample**<br>*(bagging_fraction)* | Fraction of data (rows) used per iteration. | • Faster training.<br>• **Reduces Overfitting.** | • Uses all data rows. | **Large Data:** Use 0.7~0.8 to speed up training and prevent overfitting. |\n",
    "| **bagging_freq** | Frequency for bagging (0 means disable). | • Enables bagging logic. | • Disables bagging (if 0). | Set to `1` (or k) to enable `subsample`. Essential if you use `subsample`. |\n",
    "| **cat_smooth** | Smoothing for categorical features. | • Reduces noise in categories.<br>• Regularization for rare categories. | • Sensitive to categorical noise. | **Categorical Data:** Increase (e.g., 20~50) if the model overfits on rare category values. |\n",
    "| **class_weight** | Handling imbalanced classes. | • Focuses on minority class.<br>• Improves Recall. | • Treats classes equally (if None). | **Imbalanced Data:** Set to `'balanced'` for multi-class problems. |\n",
    "| **max_bin** | Max number of bins for feature values. | • More fine-grained splits.<br>• Slower, Overfitting risk. | • Faster training.<br>• Generalization. | Decrease (e.g., 63) to speed up training on massive data. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b2a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning with LightGBM...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1036\n",
      "[LightGBM] [Info] Number of data points in the train set: 208480, number of used features: 39\n",
      "[LightGBM] [Info] Start training from score -2.339173\n",
      "[LightGBM] [Info] Start training from score -0.564028\n",
      "[LightGBM] [Info] Start training from score -1.094582\n",
      "Best Parameters: {'class_weight': None, 'max_depth': 20}\n",
      "Best CV Score: 0.6654\n",
      "\n",
      "[Test Set Evaluation]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.47      0.56      5025\n",
      "           2       0.73      0.86      0.79     29652\n",
      "           3       0.76      0.60      0.67     17444\n",
      "\n",
      "    accuracy                           0.73     52121\n",
      "   macro avg       0.72      0.64      0.67     52121\n",
      "weighted avg       0.73      0.73      0.73     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation \n",
    "# ==========================================\n",
    "X = df.copy()\n",
    "y_raw = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Initialization\n",
    "# ==========================================\n",
    "# objective='multiclass': multiclass classification\n",
    "# class_weight='balanced': for imbalanced data\n",
    "lgbm = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=3,            \n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    num_leaves=60,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Hyperparameter Grid Definition\n",
    "# ==========================================\n",
    "param_grid = {         \n",
    "    \n",
    "    'max_depth': [15, 20],    \n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Grid Search Execution\n",
    "# ==========================================\n",
    "gs = GridSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning with LightGBM...\")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results & Evaluation\n",
    "# ==========================================\n",
    "print(f\"Best Parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV Score: {gs.best_score_:.4f}\")\n",
    "\n",
    "# Predict\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 라벨 원복 (0,1,2 -> 1,2,3)\n",
    "y_test_org = le.inverse_transform(y_test)\n",
    "y_pred_org = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"\\n[Test Set Evaluation]\")\n",
    "print(classification_report(y_test_org, y_pred_org))\n",
    "\n",
    "# local CV Score : 0.6654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635b322",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "### Categorical boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f98ad",
   "metadata": {},
   "source": [
    "1. Best for dealing categorical columns\n",
    "2. very slower than GBM\n",
    "3. weak for numeric columns than GBM\n",
    "4. Very weak for sparse-matrix (like our 'has_superstructure_ \\~~~ columns or has_secondary_usage_ ~~~ columns)\n",
    "5. Many convenient black-box features like automatical hyperparameter tuning, filling missing values...\n",
    "6. Least require of hyperparameter tuning\n",
    "\n",
    "\n",
    "| Parameter | Description | If Increased | If Decreased | When You Should Tune It |\n",
    "|----------|-------------|--------------|--------------|---------------------------|\n",
    "| **iterations** | Total number of boosting rounds. | More complex model; higher chance of overfitting but higher accuracy. | Simpler model; might underfit. | Tune together with learning_rate. Increase when learning_rate is small. |\n",
    "| **learning_rate** | Step size per boosting round. | Faster learning but may overfit or become unstable. | Slower learning; requires more iterations but more stable. | Always tune. Lower lr + higher iterations gives best results. |\n",
    "| **depth** | Maximum depth of trees; affects interaction modeling. | More complex trees; risk of overfitting. | Simpler model; less expressive. | Tune when underfitting/overfitting. Usual range: 4–10. |\n",
    "| **l2_leaf_reg** | L2 regularization on leaf weights (CatBoost’s only simple regularizer). | Stronger regularization → reduces variance. | Weaker regularization → more flexible but may overfit. | Tune if overfitting and depth/iterations already balanced. |\n",
    "| **bagging_temperature** | Controls sampling randomness (0 = deterministic). | More randomness; can reduce overfitting. | Less randomness; more stable but might overfit. | Useful for large datasets or when overfitting. |\n",
    "| **rsm** | Column sampling ratio per tree (feature subsampling). | Uses more features → more complex model. | More randomness → less overfitting; can speed training. | Tune for wide/high-dimensional datasets. |\n",
    "| **leaf_estimation_iterations** | Number of gradient steps per leaf update. | More precise leaf estimation; slower but more accurate. | Faster but less accurate per tree. | Tune only for noisy or complex regression tasks. |\n",
    "| **random_strength** | Randomness added to score when selecting splits. | More randomness → prevents overfitting. | More deterministic → possibly overfits. | Use when small dataset or overfitting. |\n",
    "| **bootstrap_type** | Sampling method: Bayesian, Bernoulli, Poisson. | Affects robustness; some types fight overfitting more. | N/A | Tune mainly for imbalanced/large datasets. |\n",
    "| **scale_pos_weight** | Weight for positive class (binary). | Improves recall for minority class. | Reduces sensitivity to minority class. | Quick alternative to class_weights for binary imbalance. |\n",
    "| **auto_class_weights** | Automatic class weighting (“Balanced”). | Handles imbalance automatically. | No balancing. | When you want simple handling of imbalanced data. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01f2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning with CatBoost...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.1; total time=   4.0s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.1; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.1; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.05; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.05; total time=   3.7s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.05; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.1; total time=   6.9s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.1; total time=   6.9s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.1; total time=   6.9s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.05; total time=   7.0s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.05; total time=   8.2s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.05; total time=   8.6s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.1; total time=   6.8s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.1; total time=   6.7s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.1; total time=   7.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.05; total time=   6.4s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.05; total time=   6.5s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.05; total time=   6.5s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.1; total time=  12.3s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.1; total time=  12.3s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.1; total time=  12.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.05; total time=  12.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.05; total time=  12.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.05; total time=  12.3s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.1; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.1; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.1; total time=   4.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.05; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.05; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.05; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.1; total time=   7.5s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.1; total time=   7.5s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.1; total time=   7.6s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.05; total time=   6.9s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.05; total time=   7.6s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.05; total time=   7.7s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.1; total time=   6.4s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.1; total time=   6.3s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.1; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.05; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.05; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.05; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.1; total time=  12.0s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.1; total time=  12.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.1; total time=  12.0s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.05; total time=  12.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.05; total time=  11.9s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.05; total time=  11.9s\n",
      "Best Parameters: {'auto_class_weights': 'SqrtBalanced', 'depth': 8, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Best CV Score: 0.6786\n",
      "\n",
      "[Test Set Evaluation]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.68      0.62      5025\n",
      "           2       0.76      0.76      0.76     29652\n",
      "           3       0.71      0.67      0.69     17444\n",
      "\n",
      "    accuracy                           0.72     52121\n",
      "   macro avg       0.68      0.70      0.69     52121\n",
      "weighted avg       0.73      0.72      0.72     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.copy() \n",
    "y_raw = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Initialization\n",
    "# ==========================================\n",
    "cb = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "    random_seed=42,\n",
    "    verbose=0,                 \n",
    "    task_type='GPU',           \n",
    "    devices='0'             \n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Hyperparameter Grid Definition\n",
    "# ==========================================\n",
    "param_grid = {\n",
    "    'iterations': [500, 1000],       # n_estimators\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'depth': [6, 8],                 # very slow for deep trees\n",
    "    'auto_class_weights': ['None', 'SqrtBalanced'], # usually more stable than 'Balanced'\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Grid Search Execution\n",
    "# ==========================================\n",
    "fit_params = {\n",
    "    'cat_features': categorical_columns, \n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=cb,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning with CatBoost...\")\n",
    "gs.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results & Evaluation\n",
    "# ==========================================\n",
    "print(f\"Best Parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV Score: {gs.best_score_:.4f}\")\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "y_test_org = le.inverse_transform(y_test)\n",
    "y_pred_org = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"\\n[Test Set Evaluation]\")\n",
    "print(classification_report(y_test_org, y_pred_org))\n",
    "\n",
    "# local CV Score : 0.6786"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87860b85",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "\n",
    "using votingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2161cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Training with VotingClassifier...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .....................................weights=[1, 2]; total time=  17.9s\n",
      "[CV] END .....................................weights=[1, 2]; total time=  18.1s\n",
      "[CV] END .....................................weights=[1, 2]; total time=  19.7s\n",
      "Best Weights: {'weights': [1, 2]}\n",
      "Best Ensemble Score: 0.6803\n",
      "Training Score (Check overfitting): 0.7792\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.copy() \n",
    "y_raw = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Non-Categorical Pipeline\n",
    "# ==========================================\n",
    "rf_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('drop_cat', 'drop', categorical_columns) # remove categorical columns\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('preprocessor', rf_preprocessor),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=4\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 3. For CatBoost Classifier\n",
    "# ==========================================\n",
    "cb_clf = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.1,\n",
    "    depth=8,\n",
    "    auto_class_weights='SqrtBalanced',\n",
    "    cat_features=categorical_columns, # Important!\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    verbose=0,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. VotingClassifier definition\n",
    "# ==========================================\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_pipe),  # name, model pipeline\n",
    "        ('cb', cb_clf)    # name, model pipeline\n",
    "    ],\n",
    "    voting='soft',        # hard : majority voting, soft : probability based voting\n",
    "    n_jobs=1              \n",
    ")\n",
    "\n",
    "params = {\n",
    "    'weights': [\n",
    "        [1, 2],    \n",
    "    ]\n",
    "}\n",
    "\n",
    "grid_vote = GridSearchCV(\n",
    "    estimator=voting_clf,\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=1,  \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Ensemble Model Training with VotingClassifier...\")\n",
    "grid_vote.fit(X_train, y_train)\n",
    "best_model = grid_vote.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "train_score = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results\n",
    "# ==========================================\n",
    "print(f\"Best Weights: {grid_vote.best_params_}\")\n",
    "print(f\"Best Ensemble Score: {grid_vote.best_score_:.4f}\")\n",
    "print(f\"Training Score (Check overfitting): {train_score:.4f}\")\n",
    "\n",
    "final_model = grid_vote.best_estimator_\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "#local CV Score : 0.6804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca6b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Loading...\n",
      "Predicting with Final Ensemble Model...\n",
      "UwUSuperCute.csv Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. test data load\n",
    "# ==========================================\n",
    "print(\"Test Data Loading...\")\n",
    "test_df = pd.read_csv(\"test_values.csv\")\n",
    "building_ids = test_df[\"building_id\"] # id for submission\n",
    "\n",
    "for col in categorical_columns:\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "# ==========================================\n",
    "# 2. Final Prediction (Ensemble Model)\n",
    "# ==========================================\n",
    "print(\"Predicting with Final Ensemble Model...\")\n",
    "final_model = grid_vote.best_estimator_\n",
    "y_test_pred = final_model.predict(test_df)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Label Inverse Transformation\n",
    "# ==========================================\n",
    "\n",
    "y_test_pred_org = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# ==========================================\n",
    "# 4. CSV 저장\n",
    "# ==========================================\n",
    "submission = pd.DataFrame({\n",
    "    \"building_id\": building_ids,\n",
    "    \"damage_grade\": y_test_pred_org\n",
    "})\n",
    "\n",
    "submission.to_csv(\"UwUSuperCute.csv\", index=False)\n",
    "\n",
    "print(\"UwUSuperCute.csv Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
