{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b92599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train_values.csv\")\n",
    "\n",
    "categorical_columns = [\n",
    "     \"roof_type               \",\n",
    "     \"land_surface_condition  \",\n",
    "     \"legal_ownership_status  \",\n",
    "     \"other_floor_type        \",\n",
    "     \"position                \",\n",
    "     \"foundation_type         \",\n",
    "     \"ground_floor_type       \",\n",
    "     \"count_floors_pre_eq     \",\n",
    "     \"count_families          \",\n",
    "     \"plan_configuration      \" \n",
    "]\n",
    "\n",
    "bool_columns = [\n",
    "      \"has_superstructure_adobe_mud                \",\n",
    "      \"has_superstructure_bamboo                   \",\n",
    "      \"has_secondary_use_rental                    \", \n",
    "      \"has_secondary_use_hotel                     \", \n",
    "      \"has_secondary_use                           \",  \n",
    "      \"has_secondary_use_agriculture               \", \n",
    "      \"has_superstructure_other                    \", \n",
    "      \"has_superstructure_rc_engineered            \",  \n",
    "      \"has_superstructure_rc_non_engineered        \",  \n",
    "      \"has_superstructure_cement_mortar_stone      \",  \n",
    "      \"has_superstructure_timber                   \",  \n",
    "      \"has_superstructure_cement_mortar_brick      \",  \n",
    "      \"has_superstructure_mud_mortar_brick         \",  \n",
    "      \"has_superstructure_mud_mortar_stone         \",  \n",
    "      \"has_superstructure_stone_flag               \",  \n",
    "      \"has_secondary_use_institution               \",  \n",
    "      \"has_secondary_use_health_post               \",  \n",
    "      \"has_secondary_use_other                     \",  \n",
    "      \"has_secondary_use_use_police                \",  \n",
    "      \"has_secondary_use_gov_office                \",  \n",
    "      \"has_secondary_use_school                    \",  \n",
    "      \"has_secondary_use_industry                  \"  \n",
    "]\n",
    "categorical_columns = list(map(lambda x: x.strip(), categorical_columns))\n",
    "bool_columns = list(map(lambda x: x.strip(), bool_columns))\n",
    "\n",
    "df[categorical_columns] = df[categorical_columns].astype(\"category\")\n",
    "df[bool_columns] = df[bool_columns].astype(\"bool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894af08a",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "### base line model with bagging algorithm\n",
    "\n",
    "1. Does not require data scaling\n",
    "2. Less chance of overfitting\n",
    "3. Not highly damaged from small noise or outliers\n",
    "4. Easily find importance of columns\n",
    "\n",
    "1. Weak for catching patterns than boosting algorithms\n",
    "2. Requires large memories\n",
    "3. Can take **ONLY** float(or integer) type, never categorical or boolean, string itself. They need to be encoded to numeric.\n",
    "\n",
    "Hyperparameters\n",
    "| Name | Role | Effect When Increasing / Decreasing | dataset characteristic |\n",
    "|------|-------|-------------------------------------|-----------------------|\n",
    "| n_estimators | Number of trees | Increase: more stable, lower variance, slower.<br> Decrease: faster but higher variance. | more noise, higher this parameter |\n",
    "| max_depth | Maximum depth of each tree | Increase: deeper, more complex, more overfitting.<br> Decrease: simpler, less overfitting, higher bias. | - |\n",
    "| min_samples_split | Minimum samples to split a node | Increase: fewer splits, simpler trees.<br> Decrease: more splits, deeper trees. | More outliers, highter this parameter |\n",
    "| min_samples_leaf | Minimum samples per leaf | Increase: smoother prediction, less overfitting.<br> Decrease: more complex tree, more overfitting. | For imbalanced dataset, it should be low enough to capture minority classes |\n",
    "| max_features | Features considered at each split | Increase: trees become similar, lower randomness.<br> Decrease: higher randomness, better generalization. | Once important features are well known, higher this parameter |\n",
    "| max_leaf_nodes | Maximum leaf nodes | Increase: more complex trees.<br> Decrease: simpler trees. | - |\n",
    "| max_samples | Samples per tree (with bootstrap) | Increase: more data per tree, less randomness.<br> Decrease: faster, more randomness. | For very large dataset, decrease for speed |\n",
    "| class_weight | Class imbalance handling | Higher minority weight: better recall, lower precision.<br> Lower weight: can ignore minority. | For imbalanced dataset, 'balanced' can take minority class work better |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97fb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning...\n",
      "Best Parameters: {'class_weight': 'balanced', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best CV Score: 0.6603\n",
      "\n",
      "[Test Set Evaluation]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.66      0.59      5025\n",
      "           2       0.75      0.74      0.75     29652\n",
      "           3       0.70      0.66      0.68     17444\n",
      "\n",
      "    accuracy                           0.71     52121\n",
      "   macro avg       0.66      0.69      0.67     52121\n",
      "weighted avg       0.71      0.71      0.71     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.drop(columns=categorical_columns)\n",
    "y = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Initialization\n",
    "# ==========================================\n",
    "# n_jobs=-1 uses all available CPU cores\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Hyperparameter Grid Definition\n",
    "# ==========================================\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],              # Number of trees\n",
    "    'min_samples_leaf': [1, 2, 4],           # Min samples at leaf node\n",
    "    'max_features': ['sqrt', 'log2'],        # Features to consider at split\n",
    "    'class_weight': [None, 'balanced'],      # Handling imbalance\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Grid Search Execution\n",
    "# ==========================================\n",
    "gs = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,                 # 5-Fold Cross Validation\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,            # Parallel processing\n",
    ")\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results & Evaluation\n",
    "# ==========================================\n",
    "print(f\"Best Parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV Score: {gs.best_score_:.4f}\")\n",
    "\n",
    "# Predict using the best model found\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\n[Test Set Evaluation]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# local CV Score : 0.6603"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635b322",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "### Categorical boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f98ad",
   "metadata": {},
   "source": [
    "1. Best for dealing categorical columns\n",
    "2. very slower than GBM\n",
    "3. weak for numeric columns than GBM\n",
    "4. Very weak for sparse-matrix (like our 'has_superstructure_ \\~~~ columns or has_secondary_usage_ ~~~ columns)\n",
    "5. Many convenient black-box features like automatical hyperparameter tuning, filling missing values...\n",
    "6. Least require of hyperparameter tuning\n",
    "\n",
    "\n",
    "| Parameter | Description | If Increased | If Decreased | When You Should Tune It |\n",
    "|----------|-------------|--------------|--------------|---------------------------|\n",
    "| **iterations** | Total number of boosting rounds. | More complex model; higher chance of overfitting but higher accuracy. | Simpler model; might underfit. | Tune together with learning_rate. Increase when learning_rate is small. |\n",
    "| **learning_rate** | Step size per boosting round. | Faster learning but may overfit or become unstable. | Slower learning; requires more iterations but more stable. | Always tune. Lower lr + higher iterations gives best results. |\n",
    "| **depth** | Maximum depth of trees; affects interaction modeling. | More complex trees; risk of overfitting. | Simpler model; less expressive. | Tune when underfitting/overfitting. Usual range: 4–10. |\n",
    "| **l2_leaf_reg** | L2 regularization on leaf weights (CatBoost’s only simple regularizer). | Stronger regularization → reduces variance. | Weaker regularization → more flexible but may overfit. | Tune if overfitting and depth/iterations already balanced. |\n",
    "| **bagging_temperature** | Controls sampling randomness (0 = deterministic). | More randomness; can reduce overfitting. | Less randomness; more stable but might overfit. | Useful for large datasets or when overfitting. |\n",
    "| **rsm** | Column sampling ratio per tree (feature subsampling). | Uses more features → more complex model. | More randomness → less overfitting; can speed training. | Tune for wide/high-dimensional datasets. |\n",
    "| **leaf_estimation_iterations** | Number of gradient steps per leaf update. | More precise leaf estimation; slower but more accurate. | Faster but less accurate per tree. | Tune only for noisy or complex regression tasks. |\n",
    "| **random_strength** | Randomness added to score when selecting splits. | More randomness → prevents overfitting. | More deterministic → possibly overfits. | Use when small dataset or overfitting. |\n",
    "| **bootstrap_type** | Sampling method: Bayesian, Bernoulli, Poisson. | Affects robustness; some types fight overfitting more. | N/A | Tune mainly for imbalanced/large datasets. |\n",
    "| **scale_pos_weight** | Weight for positive class (binary). | Improves recall for minority class. | Reduces sensitivity to minority class. | Quick alternative to class_weights for binary imbalance. |\n",
    "| **auto_class_weights** | Automatic class weighting (“Balanced”). | Handles imbalance automatically. | No balancing. | When you want simple handling of imbalanced data. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01f2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning with CatBoost...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.1; total time=   4.0s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.1; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.1; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.05; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.05; total time=   3.7s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=500, learning_rate=0.05; total time=   3.8s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.1; total time=   6.9s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.1; total time=   6.9s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.1; total time=   6.9s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.05; total time=   7.0s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.05; total time=   8.2s\n",
      "[CV] END auto_class_weights=None, depth=6, iterations=1000, learning_rate=0.05; total time=   8.6s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.1; total time=   6.8s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.1; total time=   6.7s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.1; total time=   7.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.05; total time=   6.4s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.05; total time=   6.5s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=500, learning_rate=0.05; total time=   6.5s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.1; total time=  12.3s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.1; total time=  12.3s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.1; total time=  12.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.05; total time=  12.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.05; total time=  12.2s\n",
      "[CV] END auto_class_weights=None, depth=8, iterations=1000, learning_rate=0.05; total time=  12.3s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.1; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.1; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.1; total time=   4.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.05; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.05; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=500, learning_rate=0.05; total time=   4.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.1; total time=   7.5s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.1; total time=   7.5s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.1; total time=   7.6s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.05; total time=   6.9s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.05; total time=   7.6s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=6, iterations=1000, learning_rate=0.05; total time=   7.7s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.1; total time=   6.4s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.1; total time=   6.3s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.1; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.05; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.05; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=500, learning_rate=0.05; total time=   6.2s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.1; total time=  12.0s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.1; total time=  12.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.1; total time=  12.0s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.05; total time=  12.1s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.05; total time=  11.9s\n",
      "[CV] END auto_class_weights=SqrtBalanced, depth=8, iterations=1000, learning_rate=0.05; total time=  11.9s\n",
      "Best Parameters: {'auto_class_weights': 'SqrtBalanced', 'depth': 8, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "Best CV Score: 0.6786\n",
      "\n",
      "[Test Set Evaluation]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.68      0.62      5025\n",
      "           2       0.76      0.76      0.76     29652\n",
      "           3       0.71      0.67      0.69     17444\n",
      "\n",
      "    accuracy                           0.72     52121\n",
      "   macro avg       0.68      0.70      0.69     52121\n",
      "weighted avg       0.73      0.72      0.72     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.copy() \n",
    "y_raw = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Model Initialization\n",
    "# ==========================================\n",
    "cb = CatBoostClassifier(\n",
    "    loss_function='MultiClass',\n",
    "    random_seed=42,\n",
    "    verbose=0,                 \n",
    "    task_type='GPU',           \n",
    "    devices='0'             \n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Hyperparameter Grid Definition\n",
    "# ==========================================\n",
    "param_grid = {\n",
    "    'iterations': [500, 1000],       # n_estimators\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'depth': [6, 8],                 # very slow for deep trees\n",
    "    'auto_class_weights': ['None', 'SqrtBalanced'], # usually more stable than 'Balanced'\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 4. Grid Search Execution\n",
    "# ==========================================\n",
    "fit_params = {\n",
    "    'cat_features': categorical_columns, \n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=cb,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning with CatBoost...\")\n",
    "gs.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results & Evaluation\n",
    "# ==========================================\n",
    "print(f\"Best Parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV Score: {gs.best_score_:.4f}\")\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "y_test_org = le.inverse_transform(y_test)\n",
    "y_pred_org = le.inverse_transform(y_pred)\n",
    "\n",
    "print(\"\\n[Test Set Evaluation]\")\n",
    "print(classification_report(y_test_org, y_pred_org))\n",
    "\n",
    "# local CV Score : 0.6786"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87860b85",
   "metadata": {},
   "source": [
    "### Ensemble\n",
    "\n",
    "using votingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2161cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Training with VotingClassifier...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .....................................weights=[1, 2]; total time=  17.9s\n",
      "[CV] END .....................................weights=[1, 2]; total time=  18.1s\n",
      "[CV] END .....................................weights=[1, 2]; total time=  19.7s\n",
      "Best Weights: {'weights': [1, 2]}\n",
      "Best Ensemble Score: 0.6803\n",
      "Training Score (Check overfitting): 0.7792\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation\n",
    "# ==========================================\n",
    "X = df.copy() \n",
    "y_raw = pd.read_csv(\"train_labels.csv\")[\"damage_grade\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Non-Categorical Pipeline\n",
    "# ==========================================\n",
    "rf_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('drop_cat', 'drop', categorical_columns) # remove categorical columns\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    ('preprocessor', rf_preprocessor),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=4\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 3. For CatBoost Classifier\n",
    "# ==========================================\n",
    "cb_clf = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.1,\n",
    "    depth=8,\n",
    "    auto_class_weights='SqrtBalanced',\n",
    "    cat_features=categorical_columns, # Important!\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    verbose=0,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. VotingClassifier definition\n",
    "# ==========================================\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_pipe),  # name, model pipeline\n",
    "        ('cb', cb_clf)    # name, model pipeline\n",
    "    ],\n",
    "    voting='soft',        # hard : majority voting, soft : probability based voting\n",
    "    n_jobs=1              \n",
    ")\n",
    "\n",
    "params = {\n",
    "    'weights': [\n",
    "        [1, 2],    \n",
    "    ]\n",
    "}\n",
    "\n",
    "grid_vote = GridSearchCV(\n",
    "    estimator=voting_clf,\n",
    "    param_grid=params,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=1,  \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Ensemble Model Training with VotingClassifier...\")\n",
    "grid_vote.fit(X_train, y_train)\n",
    "best_model = grid_vote.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "train_score = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "# ==========================================\n",
    "# 5. Results\n",
    "# ==========================================\n",
    "print(f\"Best Weights: {grid_vote.best_params_}\")\n",
    "print(f\"Best Ensemble Score: {grid_vote.best_score_:.4f}\")\n",
    "print(f\"Training Score (Check overfitting): {train_score:.4f}\")\n",
    "\n",
    "final_model = grid_vote.best_estimator_\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "#local CV Score : 0.6804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca6b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Loading...\n",
      "Predicting with Final Ensemble Model...\n",
      "UwUSuperCute.csv Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. test data load\n",
    "# ==========================================\n",
    "print(\"Test Data Loading...\")\n",
    "test_df = pd.read_csv(\"test_values.csv\")\n",
    "building_ids = test_df[\"building_id\"] # id for submission\n",
    "\n",
    "for col in categorical_columns:\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "# ==========================================\n",
    "# 2. Final Prediction (Ensemble Model)\n",
    "# ==========================================\n",
    "print(\"Predicting with Final Ensemble Model...\")\n",
    "final_model = grid_vote.best_estimator_\n",
    "y_test_pred = final_model.predict(test_df)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Label Inverse Transformation\n",
    "# ==========================================\n",
    "\n",
    "y_test_pred_org = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# ==========================================\n",
    "# 4. CSV 저장\n",
    "# ==========================================\n",
    "submission = pd.DataFrame({\n",
    "    \"building_id\": building_ids,\n",
    "    \"damage_grade\": y_test_pred_org\n",
    "})\n",
    "\n",
    "submission.to_csv(\"UwUSuperCute.csv\", index=False)\n",
    "\n",
    "print(\"UwUSuperCute.csv Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
